import os
import glob
import re
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecTransposeImage
import gymnasium as gym
from racecar_gym.env import RaceEnv

# ==========================================
# è¨­å®šå€
# ==========================================
# ä½ çš„æ¨¡å‹å­˜æª”è·¯å¾‘
LOG_DIR = "./logs/test_A2C/"
# æ¸¬è©¦ç”¨çš„åœ°åœ–
SCENARIO = "austria_competition"
# æ¯å€‹æ¨¡å‹è¦æ¸¬å¹¾æ¬¡
N_EVAL_EPISODES = 3
# å½±åƒè™•ç†è¨­å®š
RESIZE_DIM = (64, 64)


# ==========================================

# 1. å®šç¾©å½±åƒè™•ç† Wrapper (å¿…é ˆèˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€æ¨£)
class ImageProcessWrapper(gym.ObservationWrapper):
    def __init__(self, env, resize_dim=(64, 64)):
        super().__init__(env)
        self.resize_dim = resize_dim
        self.observation_space = gym.spaces.Box(
            low=0, high=255,
            shape=(resize_dim[0], resize_dim[1], 1),
            dtype=np.uint8
        )

    def observation(self, obs):
        import cv2
        img = np.transpose(obs, (1, 2, 0))
        img = cv2.resize(img, (self.resize_dim[1], self.resize_dim[0]), interpolation=cv2.INTER_AREA)
        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        img = np.expand_dims(img, -1)
        return img


def make_env():
    # å»ºç«‹ç’°å¢ƒ
    env = RaceEnv(
        scenario=SCENARIO,
        render_mode='rgb_array_birds_eye',
        reset_when_collision=True
    )
    env = ImageProcessWrapper(env)
    return env


def extract_steps(filename):
    """å¾æª”åè§£ææ­¥æ•¸ï¼Œä¾‹å¦‚ model_ckpt_600000_steps.zip -> 600000"""
    match = re.search(r'_(\d+)_steps', filename)
    if match:
        return int(match.group(1))
    return -1


def main():
    # 1. æœå°‹æ‰€æœ‰æ¨¡å‹æª”æ¡ˆ
    # æ”¯æ´å…©ç¨®å‘½åæ ¼å¼: "PPO_t.zip" (æœ€çµ‚ç‰ˆ) æˆ– "model_ckpt_123_steps.zip"
    files = glob.glob(os.path.join(LOG_DIR, "*.zip"))

    if not files:
        print(f"âŒ åœ¨ {LOG_DIR} æ‰¾ä¸åˆ°ä»»ä½• .zip æª”æ¡ˆ")
        return

    # å»ºç«‹æ¸¬è©¦ç’°å¢ƒ
    print("ğŸ› ï¸ æ­£åœ¨å»ºç«‹æ¸¬è©¦ç’°å¢ƒ...")
    env = DummyVecEnv([make_env])
    env = VecFrameStack(env, n_stack=8)  # æ³¨æ„: é€™è£¡è¦è·Ÿè¨“ç·´æ™‚çš„ n_stack ä¸€æ¨£ (4 æˆ– 8)
    env = VecTransposeImage(env)

    results = []

    print(f"ğŸ” æ‰¾åˆ° {len(files)} å€‹æ¨¡å‹ï¼Œé–‹å§‹è©•ä¼°...")

    # æ’åºæª”æ¡ˆ (æŒ‰æ­¥æ•¸)
    files.sort(key=extract_steps)

    for model_path in files:
        steps = extract_steps(os.path.basename(model_path))

        # å¦‚æœè§£æä¸å‡ºæ­¥æ•¸ (ä¾‹å¦‚ final_model.zip)ï¼Œå°±è¨­ç‚ºæœ€å¤§å€¼æˆ–å¿½ç•¥
        if steps == -1:
            if "final" in model_path:
                steps = 999999999  # è¦–ç‚ºæœ€å¾Œ
            else:
                continue

        print(f"   -> æ­£åœ¨æ¸¬è©¦æ¨¡å‹: {os.path.basename(model_path)} (Steps: {steps})")

        try:
            # è¼‰å…¥æ¨¡å‹
            model = PPO.load(model_path, env=env)

            # é€²è¡Œè©•ä¼°
            mean_reward, std_reward = evaluate_policy(
                model,
                env,
                n_eval_episodes=N_EVAL_EPISODES,
                deterministic=True
            )

            print(f"      åˆ†æ•¸: {mean_reward:.2f} +/- {std_reward:.2f}")

            results.append({
                "Steps": steps,
                "Mean Reward": mean_reward,
                "Std Reward": std_reward,
                "Model": os.path.basename(model_path)
            })

        except Exception as e:
            print(f"      âŒ è¼‰å…¥å¤±æ•—: {e}")

    env.close()

    if not results:
        print("æ²’æœ‰æˆåŠŸè©•ä¼°ä»»ä½•æ¨¡å‹ã€‚")
        return

    # 2. è½‰ç‚º DataFrame ä¸¦ç¹ªåœ–
    df = pd.DataFrame(results)
    # éæ¿¾æ‰æ¥µç«¯å€¼ (å¦‚æœéœ€è¦)
    # df = df[df['Steps'] < 100000000]

    print("\nğŸ“Š è©•ä¼°çµæœ:")
    print(df.sort_values(by="Mean Reward", ascending=False).head())

    # ç¹ªåœ–
    plt.figure(figsize=(12, 6))
    sns.set_theme(style="darkgrid")

    # ç•«ä¸»ç·š
    sns.lineplot(data=df, x="Steps", y="Mean Reward", marker="o", linewidth=2.5)

    # ç•«æ¨™æº–å·®é™°å½±
    plt.fill_between(
        df["Steps"],
        df["Mean Reward"] - df["Std Reward"],
        df["Mean Reward"] + df["Std Reward"],
        alpha=0.2
    )

    plt.title(f"Model Performance vs Training Steps ({SCENARIO})", fontsize=16)
    plt.xlabel("Training Steps", fontsize=12)
    plt.ylabel("Average Episode Reward", fontsize=12)

    # æ¨™ç¤ºæœ€é«˜åˆ†é»
    best_row = df.loc[df['Mean Reward'].idxmax()]
    plt.annotate(
        f'Best: {best_row["Mean Reward"]:.1f}',
        xy=(best_row['Steps'], best_row['Mean Reward']),
        xytext=(best_row['Steps'], best_row['Mean Reward'] + 50),
        arrowprops=dict(facecolor='red', shrink=0.05),
    )

    save_path = "checkpoint_evaluation.png"
    plt.savefig(save_path, dpi=300)
    print(f"âœ… åœ–è¡¨å·²å„²å­˜è‡³: {save_path}")
    plt.show()


if __name__ == "__main__":
    main()